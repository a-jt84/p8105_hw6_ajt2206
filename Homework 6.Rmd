---
title: "Homework 6"
author: "Andy Turner"
date: "2023-11-26"
output: github_document
---

#  Libraries
```{r}
library(tidyverse)
library(modelr)
library(mgcv)
library(purrr)
```


# Problem 2

### Running initial code provided: Downloading the data
```{r setup, include=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```
Code above is to import and do initial cleaning on our weather data. 

Bootstrapping
```{r}
boot_straps = 
  weather_df |> 
  modelr::bootstrap(n = 5000)

boot_straps |> pull(strap) |> nth(1) |> as_tibble()

combo = weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    r_squared = map(models, broom::glance)|> 
               map(\(x) pull(x, r.squared)),
    results = map(models, broom::tidy)
  ) |> 
  unnest(r_squared, results) |> 
  janitor::clean_names() |> 
  select(id, r_squared, term, estimate) |> 
  pivot_wider(names_from = term, values_from = estimate) |> 
  group_by(id) |> 
  mutate(
    log_12= log(tmin) + log(prcp)
  )
```
**Description of Process**: I used the `modelr::bootstrap` function to more quickly perform bootstrapping. 

**Initial r_squared and log_12 descriptions** From brief glance at dataset, we have very high r_squared values throughout the bootstrapped dataframe. Regarding, log_12 values or the Log(Beta1*Beta2) there are R `sum(is.nan(combo[["log_12"]]))`." Non-Number values that show up due to the negative values of Beta_2 (the coeff for the prcp term).

```{r}
combo |> 
  ggplot(aes(x = r_squared)) +
  geom_density() +
  labs(title = "Density Plot for R-Squared") +
  theme_minimal()

combo |> 
  ggplot(aes(x = log_12)) +
  geom_density() +
  labs(title = "Density Plot for Log(Beta1*Beta2)") +
  theme_minimal()
```
**Description of R-Squared Plot**: the r-squared density plot is slightly left skewed and unimodal. the majority of the data occurs between a value of 0.90 and 0.94 with the largest peak at ~0.92. These are incredibly high r-squared values, but if we think about our model with daily temp maximum as our outcome and daily temp minimum as a predictor this makes intuitive sense. We would expect minimum and maximum daily valeus to be extremely related. 

**Description of Log(Beta1*Beta2)**: the density plot for Log(Beta1*Beta2) is extremely left skewed and unimodal. The peak of the data is approximately -5.5 with the vast majority of observations lying between -8 and -4. As mentioned, there is an extremely long left tail to the data. 

```{r}
with(combo, quantile(r_squared, c(0.025, 0.50, 0.975), na.rm = TRUE))

with(combo, quantile(log_12, c(0.025, 0.50, 0.975), na.rm = TRUE))
```
**Note on 95% CI**: In order to provide a 95% CI for r_squared and log(beta1*beta2), i used the `quantile` function. I pulled the quantile value for 0.025 and 0.975 which represent the lower and upper confidence intervals. Additionally, I pulled a point estimate for the mean value of each using a quantile of 0.50. Not asked for in problem, but I believe it made more sense to see all three numbers with each other. 

# Problem 3

**Importing Data**
```{r}
birthweight <- read.csv("data/birthweight.csv")

birthweight= 
  birthweight |> 
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    malform= factor(malform, levels= c(0,1), labels= c("absent", "present")),
    frace = case_when(
      frace == 1 ~ "White",
      frace == 2 ~ "Black",
      frace == 3 ~ "Asian",
      frace == 4 ~ "Puerto Rican",
      frace == 8 ~ "Other",
      frace == 9 ~ "Unknown",
      TRUE ~ as.character(frace)  # Keep other values as is
    ),
    mrace = case_when(
      mrace == 1 ~ "White",
      mrace == 2 ~ "Black",
      mrace == 3 ~ "Asian",
      mrace == 4 ~ "Puerto Rican",
      mrace == 8 ~ "Other",
      TRUE ~ as.character(mrace)  # Keep other values as is
    )
  ) |> 
  select(!c("pnumlbw", "pnumsga")) |> 
  filter(menarche > 4)
```
**Process**: To import the data I used `read.csv`. For the data itself: babysex, frace, malform. and mrace were originally in numeric/integer form, but made more sense functionally to be treated as Factors or Character variables. Babysex and malform were converted to a binary factor while frace and mrace were converted to a string variable (since race doesn't have inherent order). I removed pnulbw and pnumgsa from the dataset as neither had any value. I did a brief combing through of the data to ensure that all observations looked relatively realistic (given that I am not an OB/GYN). I decided to limit menarche to above 4 as from brief online search 5 seems to be the earliest accepted time to have first menstration. Only excluded 1 observation with a menerache of 0 which seems like a data error, so exclusion seems optimal. 

**Description of data**: The dataset describes `birthweight |> nrow()` babies with `birthweight |> nrow()` describing either the baby at birth themselves, or characteristics of their parents. Key variables include bwt: babies birthweight in grams, gaweeks: gestational age in weeks, and delwt: mother's weight at birth in lbs. 


```{r}
fit= lm(bwt ~ gaweeks + babysex + mheight + ppbmi + blength, data= birthweight)

broom::tidy(fit)

modelr::add_residuals(birthweight, fit)
modelr::add_predictions(birthweight, fit)

birthweight |> 
  modelr::add_residuals(fit) |> 
  modelr::add_predictions(fit) |> 
  ggplot(aes(x= pred, y=resid))+geom_point()

```
**Modelling Process*: in order to select variables for my model, I decided to briefly search for common predictors of baby's birthweight. The general consensus was that gestational age, fetal sex, mother's height, mother's pre-pregnancy BMI, and pre-partum births were commonly associated with birthweight; therefore, I added the first four variables into my model as I had their data readily available. I also decided to add in baby's birth length since it just made sense to me that length would heavily impact height. Because outcome is continuous, I decided on using a Linear Regression model. Babysex was already coded as a factor above, so it was fine to add right into the model. The other variables were treated as continuous predictors. 

**Residual Plot Description**: there does appear to be a slight pattern of the residuals with large positive residuals from 1000 to 2000 and large negative residuals from 4000 to 5000. Ideally if I was going through the full modeling process, I would want to better account for this potential pattern and also take a good look into some of the predicted values that do not make sex (Ex: -537 as a predicted birthweight). 

**Cross-Validation**
```{r}
cv_df = 
  crossv_mc(birthweight, 100)

cv_df |> pull(train) |> nth(1) |> as_tibble()
cv_df |> pull(test) |> nth(1) |> as_tibble()

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    my_mod  = map(train, \(df) lm(bwt ~ gaweeks + babysex + mheight + ppbmi + blength, data= df)),
    simple_mod  = map(train, \(df) lm(bwt ~ gaweeks+ blength, 
                                  data= df)),
    interaction_mod  = map(train, \(df) lm(bwt ~ bhead*babysex*blength, 
                                  data= df))) |> 
  mutate(
    rmse_my = map2_dbl(my_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_simple = map2_dbl(simple_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interaction = map2_dbl(interaction_mod, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
**Analysis of RMSE**: My model sucked - a lot worse than I was expecting honestly. From looking at the RMSE Violin Plot we see that my model was marginally better than the simple model whereas the interaction based model had far lower rmse values signifying that the regression line was a better fit for the data. I did not expect my model to be as far off as it is. 
